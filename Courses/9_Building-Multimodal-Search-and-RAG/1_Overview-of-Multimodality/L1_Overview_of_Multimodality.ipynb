{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5094e2a9",
   "metadata": {},
   "source": [
    "# L1: Overview of Multimodality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac731e36",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1317eb",
   "metadata": {},
   "source": [
    "* In this classroom, the libraries have been already installed for you.\n",
    "* If you would like to run this code on your own machine, you need to install the following:\n",
    "```\n",
    "    !pip install -q accelerate torch\n",
    "    !pip install -U scikit-learn\n",
    "    !pip install umap-learn\n",
    "    !pip install tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0655dd",
   "metadata": {
    "height": 46
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737a0b6-8b8a-41f3-8bd1-76baecb661c5",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74e014-9afe-4d60-86c5-2d6b0bb32126",
   "metadata": {
    "height": 369
   },
   "outputs": [],
   "source": [
    "# Import neural network training libraries\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Import basic computation libraries along with data visualization and plotting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import umap.plot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "# Import our data class which will organize MNIST and provide anchor, positive and negative samples.\n",
    "from mnist_dataset import MNISTDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae7286",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access Utils File and Helper Functions:</b> To access the <code>mnist_dataset.py</code> and other files for this notebook, 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd03013-48a7-45b8-960b-2215cab6b1b0",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd0a70-bc4f-4a83-b9a3-d13cbf11832a",
   "metadata": {
    "height": 250
   },
   "outputs": [],
   "source": [
    "# Load data from csv\n",
    "data = pd.read_csv('digit-recognizer/train.csv')\n",
    "val_count = 1000\n",
    "# common transformation for both val and train\n",
    "default_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "# Split data into val and train\n",
    "dataset = MNISTDataset(data.iloc[:-val_count], default_transform)\n",
    "val_dataset = MNISTDataset(data.iloc[-val_count:], default_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d04d7d-2f75-4cfb-834b-94b4b4b6cc46",
   "metadata": {},
   "source": [
    "## Setup our DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d12536-4a4d-4be8-b2f7-8c4f50a0b5ef",
   "metadata": {
    "height": 301
   },
   "outputs": [],
   "source": [
    "# Create torch dataloaders\n",
    "trainLoader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16, # feel free to modify this value\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=100\n",
    ")\n",
    "\n",
    "valLoader = DataLoader(val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06437287",
   "metadata": {},
   "source": [
    "### Visualize datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60b51c-7759-451c-a3e0-c2bfa9063ea6",
   "metadata": {
    "height": 454
   },
   "outputs": [],
   "source": [
    "# Function to display images with labels\n",
    "def show_images(images, title=''):\n",
    "    num_images = len(images)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(9, 3))\n",
    "    for i in range(num_images):\n",
    "        img = np.squeeze(images[i])\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some examples\n",
    "for batch_idx, (anchor_images, contrastive_images, distances, labels) in enumerate(trainLoader):\n",
    "    # Convert tensors to numpy arrays\n",
    "    anchor_images = anchor_images.numpy()\n",
    "    contrastive_images = contrastive_images.numpy()\n",
    "    labels = labels.numpy()\n",
    "    \n",
    "    # Display some samples from the batch\n",
    "    show_images(anchor_images[:4], title='Anchor Image')\n",
    "    show_images(contrastive_images[:4], title='+/- Example')\n",
    "    \n",
    "    # Break after displaying one batch for demonstration\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8f475",
   "metadata": {},
   "source": [
    "> Note: Please be aware that the output from the previous cell may differ from what is shown in the video. This variation is normal and should not cause concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a63e6a7-55b6-45cf-9756-d67263bf14b5",
   "metadata": {},
   "source": [
    "## Build Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5cfe41-4021-432f-b9a8-592f2590a803",
   "metadata": {
    "height": 573
   },
   "outputs": [],
   "source": [
    "# Define a neural network architecture with two convolution layers and two fully connected layers\n",
    "# Input to the network is an MNIST image and Output is a 64 dimensional representation. \n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2, 2), stride=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2, 2), stride=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 64),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # x: d * 32 * 12 * 12\n",
    "        x = self.conv2(x) # x: d * 64 * 4  * 4 \n",
    "        x = x.view(x.size(0), -1) # x: d * (64*4*4)\n",
    "        x = self.linear1(x) # x: d * 64\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9a3c7-60e2-4cd9-87e4-faeecfbdef51",
   "metadata": {},
   "source": [
    "## Contrastive Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1ddd8-886a-40db-9a4a-c9eba9652328",
   "metadata": {
    "height": 216
   },
   "outputs": [],
   "source": [
    "# The ideal distance metric for a positive sample is set to 1, for a negative sample it is set to 0      \n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.similarity = nn.CosineSimilarity(dim=-1, eps=1e-7)\n",
    "\n",
    "    def forward(self, anchor, contrastive, distance):\n",
    "        # use cosine similarity from torch to get score\n",
    "        score = self.similarity(anchor, contrastive)\n",
    "        # after cosine apply MSE between distance and score\n",
    "        return nn.MSELoss()(score, distance) #Ensures that the calculated score is close to the ideal distance (1 or 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da33ff",
   "metadata": {},
   "source": [
    "#### Define the Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7df6d-d9c8-439f-b65b-c03917dcd1b5",
   "metadata": {
    "height": 148
   },
   "outputs": [],
   "source": [
    "net = Network()\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    \n",
    "net = net.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45aec13-99ce-47a7-855f-90cdb4f47635",
   "metadata": {
    "height": 97
   },
   "outputs": [],
   "source": [
    "# Define the training configuration\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "loss_function = ContrastiveLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430a82c-8c02-42c9-9ccb-2ee8cf27aa91",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17170a1a",
   "metadata": {
    "height": 148
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define a directory to save the checkpoints\n",
    "checkpoint_dir = 'checkpoints/'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f3db2",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b284718",
   "metadata": {
    "height": 624
   },
   "outputs": [],
   "source": [
    "def train_model(epoch_count=10):#\n",
    "    net = Network()\n",
    "    lrs = []\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "        epoch_loss = 0\n",
    "        batches=0\n",
    "        print('epoch -', epoch)\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        print('learning rate', lrs[-1])\n",
    "    \n",
    "        for anchor, contrastive, distance, label in tqdm(trainLoader):\n",
    "            batches += 1\n",
    "            optimizer.zero_grad()\n",
    "            anchor_out = net(anchor.to(device))\n",
    "            contrastive_out = net(contrastive.to(device))\n",
    "            distance = distance.to(torch.float32).to(device)\n",
    "            loss = loss_function(anchor_out, contrastive_out, distance)\n",
    "            epoch_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(epoch_loss.cpu().detach().numpy() / batches)\n",
    "        scheduler.step()\n",
    "        print('epoch_loss', losses[-1])\n",
    "    \n",
    "        # Save a checkpoint of the model\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch}.pt')\n",
    "        torch.save(net.state_dict(), checkpoint_path)\n",
    "\n",
    "    return {\n",
    "        \"net\": net,\n",
    "        \"losses\": losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b634c",
   "metadata": {},
   "source": [
    ">Note: The model presented above is ready for training; however, please be aware that it may take several minutes to train. As a backup plan, we provide the option to load a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe982cb",
   "metadata": {},
   "source": [
    "### Load from Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e497570",
   "metadata": {
    "height": 148
   },
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint():\n",
    "    checkpoint = torch.load('checkpoints/model_epoch_99.pt')\n",
    "    \n",
    "    net = Network()\n",
    "    net.load_state_dict(checkpoint)\n",
    "    net.eval()\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b926e0b",
   "metadata": {},
   "source": [
    "Set the `train` variable to `TRUE` if you'd like to train the model, otherwise you will load a trained checkpoint of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5804b84",
   "metadata": {},
   "source": [
    "### Get the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c1ac1-5d22-48b5-ba9c-bf0aed3aebfe",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> <b>(Note: <code>train = False</code>):</b> We've saved the trained model and are loading it here for speedier results, allowing you to observe the outcomes faster. Once you've done an initial run, you may set <code>train</code> to <code>True</code> to train the model yourself. This can take some time to finsih, depending the value you set for the <code>epoch_count</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001b3d8",
   "metadata": {
    "height": 131
   },
   "outputs": [],
   "source": [
    "train = False # set to True to run train the model\n",
    "\n",
    "if train:\n",
    "    training_result = train_model()\n",
    "    model = training_result[\"net\"]\n",
    "else:\n",
    "    model = load_model_from_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597dac6e-f50a-44ad-8eb8-ede44b39daad",
   "metadata": {},
   "source": [
    "### Visualize the loss curve for your trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000500f5",
   "metadata": {
    "height": 199
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "if train:\n",
    "    # show loss curve from your training.\n",
    "    plt.plot(training_result[\"losses\"])\n",
    "    plt.show()\n",
    "else:\n",
    "    # If you are loading a checkpoint instead of training the model (train = False),\n",
    "    # the following line will show a pre-saved loss curve from the checkpoint data.\n",
    "    display(Image(filename=\"images/loss-curve.png\", height=600, width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f97aa6-6801-485a-81c7-b108099c8dc5",
   "metadata": {},
   "source": [
    "## Visualize the Vector Space!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef70ea",
   "metadata": {},
   "source": [
    "### Generate 64d Representations of the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc915c4-77eb-4350-9c45-2fc9b08a0806",
   "metadata": {
    "height": 199
   },
   "outputs": [],
   "source": [
    "encoded_data = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for anchor, _, _, label in tqdm(trainLoader):\n",
    "        output = model(anchor.to(device))\n",
    "        encoded_data.extend(output.cpu().numpy())\n",
    "        labels.extend(label.cpu().numpy())\n",
    "\n",
    "encoded_data = np.array(encoded_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ba9b8",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality of Data: 64d -> 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5aff7d",
   "metadata": {
    "height": 80
   },
   "outputs": [],
   "source": [
    "# Apply PCA to reduce dimensionality of data from 64d -> 3d to make it easier to visualize!\n",
    "pca = PCA(n_components=3)\n",
    "encoded_data_3d = pca.fit_transform(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e60de9-f098-41de-a800-b8bb9bb36947",
   "metadata": {},
   "source": [
    "### Interactive Scatter Plot in 3d ‚Äì with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311e932-db04-4dac-8581-3f0839a81b4b",
   "metadata": {
    "height": 488
   },
   "outputs": [],
   "source": [
    "scatter = go.Scatter3d(\n",
    "    x=encoded_data_3d[:, 0],\n",
    "    y=encoded_data_3d[:, 1],\n",
    "    z=encoded_data_3d[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=4, color=labels, colorscale='Viridis', opacity=0.8),\n",
    "    text=labels, \n",
    "    hoverinfo='text',\n",
    ")\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(\n",
    "    title=\"MNIST Dataset - Encoded and PCA Reduced 3D Scatter Plot\",\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"PC1\"),\n",
    "        yaxis=dict(title=\"PC2\"),\n",
    "        zaxis=dict(title=\"PC3\"),\n",
    "    ),\n",
    "    width=1000, \n",
    "    height=750,\n",
    ")\n",
    "\n",
    "# Create figure and add scatter plot\n",
    "fig = go.Figure(data=[scatter], layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94994a5-9ab2-4455-804a-8eeb565a94d9",
   "metadata": {},
   "source": [
    "### Scatterplot in 2d - with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdaadde-b9c9-44fd-9224-d2f84e285f6f",
   "metadata": {
    "height": 63
   },
   "outputs": [],
   "source": [
    "mapper = umap.UMAP(random_state=42, metric='cosine').fit(encoded_data)\n",
    "umap.plot.points(mapper, labels=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfc9fe",
   "metadata": {},
   "source": [
    "> Note: Please be aware that the output from the previous cell may differ from what is shown in the video. This variation is normal and should not cause concern. Also the previous cell might take some minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665263a8",
   "metadata": {},
   "source": [
    "### UMAP with Euclidian Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628a61a",
   "metadata": {
    "height": 46
   },
   "outputs": [],
   "source": [
    "mapper = umap.UMAP(random_state=42).fit(encoded_data) \n",
    "umap.plot.points(mapper, labels=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f28839e",
   "metadata": {},
   "source": [
    "> Note: Please be aware that the output from the previous cell may differ from what is shown in the video. This variation is normal and should not cause concern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69899a-65c8-4e7e-bb86-2a96ed29f031",
   "metadata": {},
   "source": [
    "## Contrastive Training over 100 Epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9695ff1-7e4c-4a70-bc64-e9f3627a7563",
   "metadata": {
    "height": 63
   },
   "outputs": [],
   "source": [
    "# Run it to see the training!\n",
    "from IPython.display import Video\n",
    "Video(\"contrastive_Training_100.mp4\", height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f801bb5f-b04e-4e1e-8bed-5f642a596803",
   "metadata": {
    "height": 29
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
